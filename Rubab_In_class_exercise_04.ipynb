{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rubab In-class-exercise-04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubabshz/Rubab_INFO5731_Spring2020/blob/main/Rubab_In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuX00KHNeSpw"
      },
      "source": [
        "# **The fourth in-class-exercise (20 points in total, 2/9/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vTOb03hG1f"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/info5731_spring2021/blob/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data (4 points)\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR0L3_CreM_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "357b9ef8-44ef-4cec-f33e-b4cf4457c4dd"
      },
      "source": [
        "###################################### 1.1 ######################################\n",
        "#Number of sentences\n",
        "#Number of words\n",
        "#Number of characters\n",
        "#Average word length\n",
        "#Number of stopwords\n",
        "#Number of special characters\n",
        "#Number of numerics\n",
        "#Number of uppercase words\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-n0n7ZMbZfI",
        "outputId": "58085ae1-8224-4858-8cbc-8c818eebd02a"
      },
      "source": [
        "! pip install beautifulsoup4\r\n",
        "! pip install pandas\r\n",
        "import urllib.request\r\n",
        "import pandas as pd\r\n",
        "import nltk \r\n",
        "import re\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "\r\n"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "PmAEYoZFHzoP",
        "outputId": "f9aae8df-e293-4e8b-a554-f7645f850d90"
      },
      "source": [
        "\n",
        "########################### 1.1 ################################################\n",
        "link = \"/content/legal_case.csv\"\n",
        "#page = urllib.request.urlopen(link)\n",
        "tanner=pd.read_csv(link, error_bad_lines=False, names=['Sentences'])\n",
        "\n",
        "############################### Number of Sentences ############################\n",
        "print(\"Total sentences in the file are :\", len(tanner['Sentences']))\n",
        "\n",
        "############################### Number of Words ################################\n",
        "tanner['Word_count'] = tanner['Sentences'].apply(lambda x: len(str(x).split(\" \")))\n",
        "#tanner[['Sentences']].head()\n",
        "#text_data['char_count'] =data['Text'].str.len()\n",
        "tanner[['Sentences','Word_count']].head()\n",
        "\n",
        "##############################  Number of characters  ##########################\n",
        "tanner['char_count'] = tanner['Sentences'].str.len()\n",
        "tanner[['Sentences', 'Word_count', 'char_count']].head()\n",
        "\n",
        "\n",
        "############################## Average Word Length #############################\n",
        "def avg_len(sentence):\n",
        "    words = sentence.split()\n",
        "    if len(words)!=0:\n",
        "       return (sum(len(word) for word in words)/len(words))\n",
        "    else:\n",
        "       return 0\n",
        "\n",
        "tanner['avg_len'] = tanner['Sentences'].apply(lambda x: avg_len(x))\n",
        "tanner[['Sentences', 'Word_count', 'char_count', 'avg_len']].head()\n",
        "\n",
        "########################### Number of Stop words ###############################\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "tanner['stop_words'] = tanner['Sentences'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
        "tanner[['Sentences', 'Word_count', 'char_count', 'avg_len', 'stop_words']].head()\n",
        "\n",
        "########################  Number of Special Characters #########################\n",
        "\n",
        "tanner['special_char'] = tanner[\"Sentences\"].apply(lambda x: len([x for x in x.split() if x.startswith(('#','@', '!'))]))     \n",
        "tanner[['Sentences', 'Word_count', 'char_count', 'avg_len', 'stop_words', 'special_char']].head()\n",
        "\n",
        "\n",
        "########################  Number of numerics ####################################\n",
        "\n",
        "tanner['numerics'] = tanner['Sentences'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
        "tanner[['Sentences', 'Word_count', 'char_count', 'avg_len', 'stop_words', 'numerics']].head()\n",
        "\n",
        "####################### Number of Upper case words #############################\n",
        "\n",
        "tanner['U_case'] = tanner['Sentences'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
        "tanner[['Sentences', 'Word_count', 'char_count', 'avg_len', 'stop_words', 'numerics','U_case']].head()\n",
        "\n",
        "\n"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total sentences in the file are : 148\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentences</th>\n",
              "      <th>Word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>avg_len</th>\n",
              "      <th>stop_words</th>\n",
              "      <th>numerics</th>\n",
              "      <th>U_case</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5 Ala. 740</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>2.666667</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court of Alabama.</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ADAMS</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v.</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TANNER AND HORTON.</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>5.333333</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Sentences  Word_count  ...  numerics  U_case\n",
              "0                 5 Ala. 740           3  ...         2       0\n",
              "1  Supreme Court of Alabama.           4  ...         0       0\n",
              "2                      ADAMS           1  ...         0       1\n",
              "3                         v.           1  ...         0       0\n",
              "4         TANNER AND HORTON.           3  ...         0       3\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vVNujGmGlu4w",
        "outputId": "624dcd21-103a-494c-b367-7ab9519ed020"
      },
      "source": [
        "################################## 1.2 ###################################\r\n",
        "'''\r\n",
        "Lower casing\r\n",
        "Punctuation removal\r\n",
        "Stopwords removal\r\n",
        "Frequent words removal\r\n",
        "Rare words removal\r\n",
        "Spelling correction\r\n",
        "Tokenization\r\n",
        "Stemming\r\n",
        "Lemmatization'''"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nLower casing\\nPunctuation removal\\nStopwords removal\\nFrequent words removal\\nRare words removal\\nSpelling correction\\nTokenization\\nStemming\\nLemmatization'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCNoWk1QlvDC",
        "outputId": "c50da11f-a3ed-473a-b79e-3938e51bab5f"
      },
      "source": [
        "##################################  1.2 ################################\r\n",
        "\r\n",
        "######################## Lower casing ##################################\r\n",
        "\r\n",
        "tanner['Sentences'] = tanner['Sentences'].apply(lambda x: \" \".join(x.lower() for x in x.split()))    \r\n",
        "tanner['Sentences'].head()\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                   5 ala. 740\n",
              "1    supreme court of alabama.\n",
              "2                        adams\n",
              "3                           v.\n",
              "4           tanner and horton.\n",
              "Name: Sentences, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTfLxBwTsoxi",
        "outputId": "3b0817cc-17a0-4a20-800d-c2129313cb6e"
      },
      "source": [
        "######################## Punctuation Removal ##########################\r\n",
        "\r\n",
        "tanner['Sentences'] = tanner['Sentences'].str.replace('[^\\w\\s]','')\r\n",
        "tanner['Sentences'].head()"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                   5 ala 740\n",
              "1    supreme court of alabama\n",
              "2                       adams\n",
              "3                           v\n",
              "4           tanner and horton\n",
              "Name: Sentences, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xlvfBsessy7",
        "outputId": "35d931ed-8833-4ac6-ba06-28d86869b182"
      },
      "source": [
        "####################### Stop words removal ############################\r\n",
        "\r\n",
        "stop = stopwords.words('english')\r\n",
        "tanner['Sentences'] = tanner['Sentences'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop ))\r\n",
        "tanner['Sentences'].head()"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                5 ala 740\n",
              "1    supreme court alabama\n",
              "2                    adams\n",
              "3                        v\n",
              "4            tanner horton\n",
              "Name: Sentences, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvbryJy0sxHs",
        "outputId": "fa442d04-c555-45f7-f0e9-86ea5b65b254"
      },
      "source": [
        "###################### Frequent words removal #########################\r\n",
        "freq = pd.Series(' '.join(tanner['Sentences']).split()).value_counts()[:10]\r\n",
        "freq = list(freq.index)\r\n",
        "tanner['Sentences'] = tanner['Sentences'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\r\n",
        "tanner['Sentences'].head()"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                740\n",
              "1    supreme alabama\n",
              "2              adams\n",
              "3                   \n",
              "4      tanner horton\n",
              "Name: Sentences, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oBWg7q3sxX6",
        "outputId": "13d73661-c3ef-4906-8cd4-bc5bd424563c"
      },
      "source": [
        "###################### Rare words removal #############################\r\n",
        "\r\n",
        "freq = pd.Series(' '.join(tanner['Sentences']).split()).value_counts()[-10:]\r\n",
        "freq = list(freq.index)\r\n",
        "tanner['Sentences'] = tanner['Sentences'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\r\n",
        "tanner['Sentences'].head()"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                740\n",
              "1    supreme alabama\n",
              "2              adams\n",
              "3                   \n",
              "4      tanner horton\n",
              "Name: Sentences, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzw_0Pg6rvQz",
        "outputId": "7b989a0e-6f35-4896-853e-c2cff8f04e98"
      },
      "source": [
        "######################### Spelling correction #########################\r\n",
        "from textblob import TextBlob\r\n",
        "tanner['Sentences'] = tanner['Sentences'][:5].apply(lambda x: str(TextBlob(x).correct()))\r\n",
        "tanner['Sentences'].head()\r\n"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                740\n",
              "1    supreme alabama\n",
              "2              adams\n",
              "3                   \n",
              "4      manner norton\n",
              "Name: Sentences, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4dNDt_7sxgQ",
        "outputId": "6ab86b10-8e87-4cea-fcc0-5299ab8f8760"
      },
      "source": [
        "####################### Tokenization #################################\r\n",
        "TextBlob(tanner['Sentences'][1]).words"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['supreme', 'alabama'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zBsbD_ltfzM",
        "outputId": "d17a95f6-fc10-4f49-de7a-96a6f287ecf2"
      },
      "source": [
        "########################## Stemming ##################################\r\n",
        "st = PorterStemmer()\r\n",
        "tanner['Sentences'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0               740\n",
              "1    suprem alabama\n",
              "2              adam\n",
              "3                  \n",
              "4     manner norton\n",
              "Name: Sentences, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kqZ2JWEtf-n",
        "outputId": "9360790b-0988-4305-d538-4ced7635c4a5"
      },
      "source": [
        "\r\n",
        "######################## Lemmatization ###############################\r\n",
        "from textblob import Word\r\n",
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "tanner['Sentences'] = tanner['Sentences'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "tanner['Sentences'].head()\r\n"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                   5 Ala. 740\n",
              "1    Supreme Court of Alabama.\n",
              "2                        ADAMS\n",
              "3                           v.\n",
              "4           TANNER AND HORTON.\n",
              "Name: Sentences, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1ovJ-xiwAXP"
      },
      "source": [
        "#################### 1.3 ##############################################\r\n",
        "tanner.to_csv('Cleaned_Legal_case.csv', index=False)"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtYed9sNwAjh",
        "outputId": "59b8cbef-8340-4717-e4ba-4557971590b4"
      },
      "source": [
        "####################  1.4 ##############################################\r\n",
        "#Calculate the term frequency of all the terms.\r\n",
        "#Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\r\n",
        "\r\n",
        "############ Top 10 1-gram, 2-gram, and 3-gram terms as features ###########\r\n",
        "TextBlob(tanner['Sentences'][0]).ngrams(1)\r\n",
        "TextBlob(tanner['Sentences'][0]).ngrams(2)\r\n",
        "TextBlob(tanner['Sentences'][0]).ngrams(3)\r\n",
        "\r\n",
        "\r\n",
        "#import pandas as pd\r\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "#link1=\"Cleaned_Legal_case.csv\"\r\n",
        "#tanner=pd.read_csv(link1,error_bad_lines=False,names=['Sentences'])\r\n",
        "\r\n",
        "#tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word', stop_words= 'english', ngram_range = (1,3))\r\n",
        "#tfs = tfidf.fit_transform(tanner['Sentences'])\r\n",
        "\r\n",
        "#print(tfidf.get_feature_names())\r\n"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['5', 'Ala', '740'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "IOb65sHBzPZG",
        "outputId": "f8079350-7654-4852-a0e8-944fa4d0cf3a"
      },
      "source": [
        "############# Term Frequency #############\r\n",
        "tf1 = (tanner['Sentences'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis= 0).reset_index()\r\n",
        "tf1.columns = ['terms', 'tf']\r\n",
        "tf1"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>terms</th>\n",
              "      <th>tf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>of</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Court</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Alabama.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Supreme</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      terms  tf\n",
              "0        of   1\n",
              "1     Court   1\n",
              "2  Alabama.   1\n",
              "3   Supreme   1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QJ-UwCenvN"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSv6fVhOfFmv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8623611-6f71-485d-9b15-a3f2bc1dc718"
      },
      "source": [
        "ip = \"260.08.094.109\"\n",
        "zeros_removed = re.sub('\\.[0]*','.', ip)\n",
        "print(zeros_removed)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "260.8.94.109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXRjaHzrfKAy"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xdJpDx9gjbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22727c5f-8b8e-46df-ceb6-b67c9df2bd41"
      },
      "source": [
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\r\n",
        "extracted_years = re.findall(r'2\\d\\d\\d', sentence)\r\n",
        "print(extracted_years)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2010', '2010', '2019']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}